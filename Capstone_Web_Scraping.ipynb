{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaeR_9roIZaV"
      },
      "source": [
        "Capstone Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94Qj820dIYtY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "all_inactives = []\n",
        "gamedate = ['202111010PHI']\n",
        "for date in gamedate:\n",
        "  url = \"https://www.basketball-reference.com/boxscores/\"+ date +\".html\"\n",
        "\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  # Find the 'scorebox_meta' div\n",
        "  scorebox_meta = soup.find('div', class_='scorebox_meta')\n",
        "  # print(scorebox_meta)\n",
        "  # Extract the text from the last two divs inside 'scorebox_meta' (Game Time and Stadium)\n",
        "  data = [div.text for div in scorebox_meta.find_all('div')[:2]]\n",
        "  gametime_stadium = [data]\n",
        "\n",
        "  # Extract Attendance\n",
        "  attendance_div = soup.find('strong', string=lambda x: x and 'Attendance:' in x)\n",
        "  attendance_value = int(attendance_div.next_sibling.strip().replace(\",\", \"\"))\n",
        "  attendance_list = [attendance_value]\n",
        "\n",
        "  # Extract Records\n",
        "  scores = soup.find_all('div', class_='score')\n",
        "  away_record = scores[0].find_next('div').text\n",
        "  home_record = scores[1].find_next('div').text\n",
        "\n",
        "  # Extract Inactives\n",
        "  inactives_strong = soup.find('strong', string=lambda x: x and 'Inactive:' in x)\n",
        "  inactives_div = inactives_strong.find_parent() if inactives_strong else None\n",
        "\n",
        "  inactives = {'game_id': date, 'POR': [], 'Opponent': []}\n",
        "  if inactives_div:\n",
        "      spans = inactives_div.find_all('span')\n",
        "      for i, span in enumerate(spans):\n",
        "          team = span.strong.text if span.strong else 'Unknown'\n",
        "          players = []\n",
        "          next_tag = span.find_next_sibling()\n",
        "          while next_tag and next_tag.name == 'a':\n",
        "              players.append(next_tag.text)\n",
        "              next_tag = next_tag.find_next_sibling()\n",
        "          if team == 'POR':\n",
        "              inactives['POR'].extend(players)\n",
        "          else:\n",
        "              inactives['Opponent'].extend(players)\n",
        "\n",
        "  all_inactives.append(inactives)\n",
        "\n",
        "# Convert the collected data to a DataFrame\n",
        "df_inactives = pd.DataFrame(all_inactives)\n",
        "\n",
        "# Display the DataFrame\n",
        "df_inactives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-CwPu4ExKpB",
        "outputId": "384fce88-0359-426e-9eed-50a9572f0399"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "gamedates = [\n",
        "'202110200POR',\n",
        "'202110230POR',\n",
        "'202110250LAC',\n",
        "'202110270POR',\n",
        "'202110290POR',\n",
        "'202110310CHO',\n",
        "'202111010PHI',\n",
        "'202111030CLE',\n",
        "'202111050POR',\n",
        "'202111060POR',\n",
        "'202111090LAC',\n",
        "'202111100PHO',\n",
        "'202111120HOU',\n",
        "'202111140DEN',\n",
        "'202111150POR',\n",
        "'202111170POR',\n",
        "'202111200POR',\n",
        "'202111230POR',\n",
        "'202111240SAC',\n",
        "'202111260GSW',\n",
        "'202111290UTA',\n",
        "'202111300POR',\n",
        "'202112020POR',\n",
        "'202112040POR',\n",
        "'202112060POR',\n",
        "'202112080GSW',\n",
        "'202112120POR',\n",
        "'202112140POR',\n",
        "'202112150POR',\n",
        "'202112170CHO',\n",
        "'202112190MEM',\n",
        "'202112210NOP',\n",
        "'202112270POR',\n",
        "'202112290POR',\n",
        "'202112310LAL',\n",
        "'202201030POR',\n",
        "'202201050POR',\n",
        "'202201070POR',\n",
        "'202201090POR',\n",
        "'202201100POR',\n",
        "'202201130DEN',\n",
        "'202201150WAS',\n",
        "'202201170ORL',\n",
        "'202201190MIA',\n",
        "'202201210BOS',\n",
        "'202201230TOR',\n",
        "'202201250POR',\n",
        "'202201260POR',\n",
        "'202201280HOU',\n",
        "'202201300CHI',\n",
        "'202201310OKC',\n",
        "'202202020LAL',\n",
        "'202202040POR',\n",
        "'202202050POR',\n",
        "'202202080POR',\n",
        "'202202090POR',\n",
        "'202202120POR',\n",
        "'202202140MIL',\n",
        "'202202160MEM',\n",
        "'202202240POR',\n",
        "'202202270POR',\n",
        "'202203020PHO',\n",
        "'202203050MIN',\n",
        "'202203070MIN',\n",
        "'202203090UTA',\n",
        "'202203120POR',\n",
        "'202203140ATL',\n",
        "'202203160NYK',\n",
        "'202203180BRK',\n",
        "'202203200IND',\n",
        "'202203210DET',\n",
        "'202203230POR',\n",
        "'202203250POR',\n",
        "'202203260POR',\n",
        "'202203280POR',\n",
        "'202203300POR',\n",
        "'202204010SAS',\n",
        "'202204030SAS',\n",
        "'202204050OKC',\n",
        "'202204070NOP',\n",
        "'202204080DAL',\n",
        "'202204100POR',\n",
        "\n",
        "'202210190SAC',\n",
        "'202210210POR',\n",
        "'202210230LAL',\n",
        "'202210240POR',\n",
        "'202210260POR',\n",
        "'202210280POR',\n",
        "'202211020POR',\n",
        "'202211040PHO',\n",
        "'202211050PHO',\n",
        "'202211070MIA',\n",
        "'202211090CHO',\n",
        "'202211100NOP',\n",
        "'202211120DAL',\n",
        "'202211150POR',\n",
        "'202211170POR',\n",
        "'202211190POR',\n",
        "'202211210MIL',\n",
        "'202211230CLE',\n",
        "'202211250NYK',\n",
        "'202211270BRK',\n",
        "'202211290POR',\n",
        "'202211300LAL',\n",
        "'202212030UTA',\n",
        "'202212040POR',\n",
        "'202212080POR',\n",
        "'202212100POR',\n",
        "'202212120POR',\n",
        "'202212140SAS',\n",
        "'202212160DAL',\n",
        "'202212170HOU',\n",
        "'202212190OKC',\n",
        "'202212210OKC',\n",
        "'202212230DEN',\n",
        "'202212260POR',\n",
        "'202212300GSW',\n",
        "'202301020POR',\n",
        "'202301040MIN',\n",
        "'202301060IND',\n",
        "'202301080TOR',\n",
        "'202301100POR',\n",
        "'202301120POR',\n",
        "'202301140POR',\n",
        "'202301150POR',\n",
        "'202301170DEN',\n",
        "'202301190POR',\n",
        "'202301220POR',\n",
        "'202301230POR',\n",
        "'202301250POR',\n",
        "'202301280POR',\n",
        "'202301300POR',\n",
        "'202302010MEM',\n",
        "'202302030WAS',\n",
        "'202302040CHI',\n",
        "'202302060POR',\n",
        "'202302080POR',\n",
        "'202302100POR',\n",
        "'202302130POR',\n",
        "'202302140POR',\n",
        "'202302230SAC',\n",
        "'202302260POR',\n",
        "'202302280GSW',\n",
        "'202303010POR',\n",
        "'202303030ATL',\n",
        "'202303050ORL',\n",
        "'202303060DET',\n",
        "'202303080BOS',\n",
        "'202303100PHI',\n",
        "'202303120NOP',\n",
        "'202303140POR',\n",
        "'202303170POR',\n",
        "'202303190POR',\n",
        "'202303220UTA',\n",
        "'202303240POR',\n",
        "'202303260POR',\n",
        "'202303270POR',\n",
        "'202303290POR',\n",
        "'202303310POR',\n",
        "'202304020MIN',\n",
        "'202304040MEM',\n",
        "'202304060SAS',\n",
        "'202304080LAC',\n",
        "'202304090GSW'\n",
        "\n",
        "]\n",
        "all_data = []\n",
        "all_inactives = []\n",
        "\n",
        "for date in gamedates:\n",
        "    url = \"https://www.basketball-reference.com/boxscores/\" + date + \".html\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the 'scorebox_meta' div\n",
        "    scorebox_meta = soup.find('div', class_='scorebox_meta')\n",
        "    if not scorebox_meta:\n",
        "        all_data.append({\n",
        "          'game_identifier': date,\n",
        "          'Game Time': 'N/A',\n",
        "          'Stadium': 'N/A',\n",
        "          'Attendance': 'N/A',\n",
        "          'Away Record': 'N/A',\n",
        "          'Home Record': 'N/A'\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # Extract the text from the last two divs inside 'scorebox_meta' (game time and stadium)\n",
        "    game_info = [div.text for div in scorebox_meta.find_all('div')[:2]]\n",
        "\n",
        "    # Extract Records\n",
        "    scores = soup.find_all('div', class_='score')\n",
        "    away_record = scores[0].find_next('div').text\n",
        "    home_record = scores[1].find_next('div').text\n",
        "\n",
        "  # Extract Inactives\n",
        "    inactives_strong = soup.find('strong', string=lambda x: x and 'Inactive:' in x)\n",
        "    inactives_div = inactives_strong.find_parent() if inactives_strong else None\n",
        "\n",
        "    inactives = {'game_id': date, 'POR': [], 'Opponent': []}\n",
        "    if inactives_div:\n",
        "        spans = inactives_div.find_all('span')\n",
        "        for i, span in enumerate(spans):\n",
        "            team = span.strong.text if span.strong else 'Unknown'\n",
        "            players = []\n",
        "            next_tag = span.find_next_sibling()\n",
        "            while next_tag and next_tag.name == 'a':\n",
        "                players.append(next_tag.text)\n",
        "                next_tag = next_tag.find_next_sibling()\n",
        "            if team == 'POR':\n",
        "                inactives['POR'].extend(players)\n",
        "            else:\n",
        "                inactives['Opponent'].extend(players)\n",
        "\n",
        "    all_inactives.append(inactives)\n",
        "    # Extract Attendance\n",
        "    attendance_div = soup.find('strong', string=lambda x: x and 'Attendance:' in x)\n",
        "    if not attendance_div:\n",
        "        all_data.append({\n",
        "            'game_identifier': date,\n",
        "            'Game Time': game_info[0],\n",
        "            'Stadium': game_info[1],\n",
        "            'Attendance': 'Error',\n",
        "            'Away Record': away_record,\n",
        "            'Home Record': home_record\n",
        "        })\n",
        "        continue\n",
        "    attendance = int(attendance_div.next_sibling.strip().replace(\",\", \"\"))\n",
        "\n",
        "    # Append data to all_data list\n",
        "    all_data.append({\n",
        "        'game_identifier': date,\n",
        "        'Game Time': game_info[0],\n",
        "        'Stadium': game_info[1],\n",
        "        'Attendance': attendance,\n",
        "        'Away Record': away_record,\n",
        "        'Home Record': home_record\n",
        "    })\n",
        "\n",
        "    time.sleep(4)\n",
        "\n",
        "# Convert the collected data to a DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "df2 = pd.DataFrame(all_inactives)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "print(df2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pd2iRVfTFDt"
      },
      "outputs": [],
      "source": [
        "# df.to_csv('game_data.csv', index=False)\n",
        "df2.to_csv('inactive_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfOeZxWxTFdm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbXa_cAN9eyU"
      },
      "outputs": [],
      "source": [
        "pip install basketball-reference-scraper==v1.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "DDH1VvyTz4U4",
        "outputId": "9cc080f7-20cb-4904-c401-81db4d80eb51"
      },
      "outputs": [],
      "source": [
        "from basketball_reference_scraper.teams import get_roster, get_team_stats, get_opp_stats, get_roster_stats, get_team_misc\n",
        "s1 = get_team_stats('GSW', 2019, data_format='PER_GAME')\n",
        "print(s1)\n",
        "\n",
        "s2 = get_opp_stats('GSW', 2019, data_format='PER_GAME')\n",
        "print(s2)\n",
        "\n",
        "s3 = get_injury_report()\n",
        "print(s3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNQXgJkCw-qh"
      },
      "source": [
        "# Collecting Additional Data Years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAw24mUTw-Il",
        "outputId": "dc47fd10-f4b1-4935-a503-4ace47c22c0d"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "gamedates = [\n",
        "\"201710180PHO\",\n",
        "\"201710200IND\",\n",
        "\"201710210MIL\",\n",
        "\"201710240POR\",\n",
        "\"201710260POR\",\n",
        "\"201710280POR\",\n",
        "\"201710300POR\",\n",
        "\"201711010UTA\",\n",
        "\"201711020POR\",\n",
        "\"201711050POR\",\n",
        "\"201711070POR\",\n",
        "\"201711100POR\",\n",
        "\"201711130POR\",\n",
        "\"201711150POR\",\n",
        "\"201711170SAC\",\n",
        "\"201711180POR\",\n",
        "\"201711200MEM\",\n",
        "\"201711220PHI\",\n",
        "\"201711240BKN\",\n",
        "\"201711250WAS\",\n",
        "\"201711270NYK\",\n",
        "\"201711300POR\",\n",
        "\"201712020POR\",\n",
        "\"201712050POR\",\n",
        "\"201712090POR\",\n",
        "\"201712110GSW\",\n",
        "\"201712130MIA\",\n",
        "\"201712150ORL\",\n",
        "\"201712160CHO\",\n",
        "\"201712180MIN\",\n",
        "\"201712200POR\",\n",
        "\"201712220POR\",\n",
        "\"201712230LAL\",\n",
        "\"201712280POR\",\n",
        "\"201712300ATL\",\n",
        "\"201801010CHI\",\n",
        "\"201801020CLE\",\n",
        "\"201801050POR\",\n",
        "\"201801070POR\",\n",
        "\"201801090OKC\",\n",
        "\"201801100HOU\",\n",
        "\"201801120NOP\",\n",
        "\"201801140MIN\",\n",
        "\"201801160POR\",\n",
        "\"201801180POR\",\n",
        "\"201801200POR\",\n",
        "\"201801220DEN\",\n",
        "\"201801240POR\",\n",
        "\"201801260DAL\",\n",
        "\"201801300LAC\",\n",
        "\"201801310POR\",\n",
        "\"201802020TOR\",\n",
        "\"201802040BOS\",\n",
        "\"201802050DET\",\n",
        "\"201802080POR\",\n",
        "\"201802090SAC\",\n",
        "\"201802110POR\",\n",
        "\"201802140POR\",\n",
        "\"201802230UTA\",\n",
        "\"201802240PHO\",\n",
        "\"201802270POR\",\n",
        "\"201803010POR\",\n",
        "\"201803030POR\",\n",
        "\"201803050LAL\",\n",
        "\"201803060POR\",\n",
        "\"201803090POR\",\n",
        "\"201803120POR\",\n",
        "\"201803150POR\",\n",
        "\"201803170POR\",\n",
        "\"201803180LAC\",\n",
        "\"201803200POR\",\n",
        "\"201803230POR\",\n",
        "\"201803250OKC\",\n",
        "\"201803270NOP\",\n",
        "\"201803280MEM\",\n",
        "\"201803300POR\",\n",
        "\"201804010POR\",\n",
        "\"201804030DAL\",\n",
        "\"201804050HOU\",\n",
        "\"201804070SAS\",\n",
        "\"201804090DEN\",\n",
        "\"201804110POR\",\n",
        "\"201810180POR\",\n",
        "\"201810200POR\",\n",
        "\"201810220POR\",\n",
        "\"201810250ORL\",\n",
        "\"201810270MIA\",\n",
        "\"201810290IND\",\n",
        "\"201810300HOU\",\n",
        "\"201811010POR\",\n",
        "\"201811030POR\",\n",
        "\"201811040POR\",\n",
        "\"201811060POR\",\n",
        "\"201811080POR\",\n",
        "\"201811110POR\",\n",
        "\"201811140LAL\",\n",
        "\"201811160MIN\",\n",
        "\"201811180WAS\",\n",
        "\"201811200NYK\",\n",
        "\"201811210MIL\",\n",
        "\"201811230GSW\",\n",
        "\"201811250POR\",\n",
        "\"201811280POR\",\n",
        "\"201811300POR\",\n",
        "\"201812020SAS\",\n",
        "\"201812040DAL\",\n",
        "\"201812060POR\",\n",
        "\"201812080POR\",\n",
        "\"201812110HOU\",\n",
        "\"201812120MEM\",\n",
        "\"201812140POR\",\n",
        "\"201812170LAC\",\n",
        "\"201812190POR\",\n",
        "\"201812210POR\",\n",
        "\"201812230POR\",\n",
        "\"201812250UTA\",\n",
        "\"201812270GSW\",\n",
        "\"201812290POR\",\n",
        "\"201812300POR\",\n",
        "\"201901010SAC\",\n",
        "\"201901040POR\",\n",
        "\"201901050POR\",\n",
        "\"201901070POR\",\n",
        "\"201901090POR\",\n",
        "\"201901110POR\",\n",
        "\"201901130DEN\",\n",
        "\"201901140SAC\",\n",
        "\"201901160POR\",\n",
        "\"201901180POR\",\n",
        "\"201901210UTA\",\n",
        "\"201901220OKC\",\n",
        "\"201901240PHO\",\n",
        "\"201901260POR\",\n",
        "\"201901300POR\",\n",
        "\"201902050POR\",\n",
        "\"201902070POR\",\n",
        "\"201902100DAL\",\n",
        "\"201902110OKC\",\n",
        "\"201902130POR\",\n",
        "\"201902210BKN\",\n",
        "\"201902230PHI\",\n",
        "\"201902250CLE\",\n",
        "\"201902270BOS\",\n",
        "\"201903010TOR\",\n",
        "\"201903030CHO\",\n",
        "\"201903050MEM\",\n",
        "\"201903070POR\",\n",
        "\"201903090POR\",\n",
        "\"201903120LAC\",\n",
        "\"201903150NOP\",\n",
        "\"201903160SAS\",\n",
        "\"201903180POR\",\n",
        "\"201903200POR\",\n",
        "\"201903230POR\",\n",
        "\"201903250POR\",\n",
        "\"201903270CHI\",\n",
        "\"201903290ATL\",\n",
        "\"201903300DET\",\n",
        "\"201904010MIN\",\n",
        "\"201904030POR\",\n",
        "\"201904050DEN\",\n",
        "\"201904070POR\",\n",
        "\"201904090LAL\",\n",
        "\"201904100POR\",\n",
        "\"201910230POR\",\n",
        "\"201910250SAC\",\n",
        "\"201910270DAL\",\n",
        "\"201910280SAS\",\n",
        "\"201910300OKC\",\n",
        "\"201911020POR\",\n",
        "\"201911040GSW\",\n",
        "\"201911070LAC\",\n",
        "\"201911080POR\",\n",
        "\"201911100POR\",\n",
        "\"201911120SAC\",\n",
        "\"201911130POR\",\n",
        "\"201911160SAS\",\n",
        "\"201911180HOU\",\n",
        "\"201911190NOP\",\n",
        "\"201911210MIL\",\n",
        "\"201911230CLE\",\n",
        "\"201911250CHI\",\n",
        "\"201911270POR\",\n",
        "\"201911290POR\",\n",
        "\"201912030LAC\",\n",
        "\"201912040POR\",\n",
        "\"201912060POR\",\n",
        "\"201912080POR\",\n",
        "\"201912100POR\",\n",
        "\"201912120DEN\",\n",
        "\"201912160PHO\",\n",
        "\"201912180POR\",\n",
        "\"201912200POR\",\n",
        "\"201912210POR\",\n",
        "\"201912230POR\",\n",
        "\"201912260UTA\",\n",
        "\"201912280POR\",\n",
        "\"201912300POR\",\n",
        "\"202001010NYK\",\n",
        "\"202001030WAS\",\n",
        "\"202001050MIA\",\n",
        "\"202001070TOR\",\n",
        "\"202001090MIN\",\n",
        "\"202001110POR\",\n",
        "\"202001130POR\",\n",
        "\"202001150HOU\",\n",
        "\"202001170DAL\",\n",
        "\"202001180OKC\",\n",
        "\"202001200POR\",\n",
        "\"202001230POR\",\n",
        "\"202001260POR\",\n",
        "\"202001290POR\",\n",
        "\"202001310LAL\",\n",
        "\"202002010POR\",\n",
        "\"202002040DEN\",\n",
        "\"202002060POR\",\n",
        "\"202002070UTA\",\n",
        "\"202002090POR\",\n",
        "\"202002110NOP\",\n",
        "\"202002120MEM\",\n",
        "\"202002210POR\",\n",
        "\"202002230POR\",\n",
        "\"202002250POR\",\n",
        "\"202002270IND\",\n",
        "\"202002290ATL\",\n",
        "\"202003020ORL\",\n",
        "\"202003040POR\",\n",
        "\"202003060PHO\",\n",
        "\"202003070POR\",\n",
        "\"202003100POR\"\n",
        "]\n",
        "all_data = []\n",
        "all_inactives = []\n",
        "\n",
        "for date in gamedates: #parsing through each game_id\n",
        "    url = \"https://www.basketball-reference.com/boxscores/\" + date + \".html\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the 'scorebox_meta' div\n",
        "    scorebox_meta = soup.find('div', class_='scorebox_meta')\n",
        "    if not scorebox_meta: # checking that it collected data\n",
        "        all_data.append({\n",
        "          'game_identifier': date,\n",
        "          'Game Time': 'N/A',\n",
        "          'Stadium': 'N/A',\n",
        "          'Attendance': 'N/A',\n",
        "          'Away Record': 'N/A',\n",
        "          'Home Record': 'N/A'\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # Extract the text from the last two divs inside 'scorebox_meta' (game time and stadium)\n",
        "    game_info = [div.text for div in scorebox_meta.find_all('div')[:2]]\n",
        "\n",
        "    # Extract Records\n",
        "    scores = soup.find_all('div', class_='score')\n",
        "    away_record = scores[0].find_next('div').text\n",
        "    home_record = scores[1].find_next('div').text\n",
        "\n",
        "  # Extract Inactives\n",
        "    inactives_strong = soup.find('strong', string=lambda x: x and 'Inactive:' in x)\n",
        "    inactives_div = inactives_strong.find_parent() if inactives_strong else None\n",
        "\n",
        "    inactives = {'game_id': date, 'POR': [], 'Opponent': []}\n",
        "    if inactives_div:\n",
        "        spans = inactives_div.find_all('span')\n",
        "        for i, span in enumerate(spans):\n",
        "            team = span.strong.text if span.strong else 'Unknown'\n",
        "            players = []\n",
        "            next_tag = span.find_next_sibling()\n",
        "            while next_tag and next_tag.name == 'a':\n",
        "                players.append(next_tag.text)\n",
        "                next_tag = next_tag.find_next_sibling()\n",
        "            if team == 'POR':\n",
        "                inactives['POR'].extend(players)\n",
        "            else:\n",
        "                inactives['Opponent'].extend(players)\n",
        "\n",
        "    all_inactives.append(inactives)\n",
        "    # Extract Attendance\n",
        "    attendance_div = soup.find('strong', string=lambda x: x and 'Attendance:' in x)\n",
        "    if not attendance_div: # Checking for nul value in attendance\n",
        "        all_data.append({\n",
        "            'game_identifier': date,\n",
        "            'Game Time': game_info[0],\n",
        "            'Stadium': game_info[1],\n",
        "            'Attendance': 'Error',\n",
        "            'Away Record': away_record,\n",
        "            'Home Record': home_record\n",
        "        })\n",
        "        continue\n",
        "    attendance = int(attendance_div.next_sibling.strip().replace(\",\", \"\"))\n",
        "\n",
        "    # Append data to all_data list\n",
        "    all_data.append({\n",
        "        'game_identifier': date,\n",
        "        'Game Time': game_info[0],\n",
        "        'Stadium': game_info[1],\n",
        "        'Attendance': attendance,\n",
        "        'Away Record': away_record,\n",
        "        'Home Record': home_record\n",
        "    })\n",
        "\n",
        "    time.sleep(4)\n",
        "\n",
        "# Convert the collected data to a DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "df2 = pd.DataFrame(all_inactives)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "print(df2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq1nRVFrIJdp"
      },
      "outputs": [],
      "source": [
        "df.to_csv('game_data2.csv', index=False)\n",
        "df2.to_csv('inactive_data2.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
